# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HfEady7IcBD_cAaqOV2EoZa0F15lOamu
"""

import tensorflow as tf
import keras.layers
import numpy as np
import logging

class LSTM(tf.keras.Model):
  """
  Multivariata, per ora multi-nodo (concateno le feature di ogni nodo tutte insieme). Predico P x N (un valore (produzione) per ogni nodo, per ogni prediction step), tramite
  un layer dense, che prende l'output della LSTM (ovvero P x N*F) e converte in P x N.
  L'output della LSTM consiste della concatenazione degli hidden state per ogni prediction step (P hidden state, ognuno da N*F).
  """
  def __init__(self, nodes, features, prediction_steps, **kwargs):
    """

    """
    super(LSTM, self).__init__()

    self.is_GRU = kwargs.get('is_GRU', False)
    self.is_bidirectional = kwargs.get('is_bidirectional', False)
    self.has_attention = kwargs.get('has_attention', False)

    self.logger = logging.getLogger(__name__)
    self.logger.info(__name__ + ' initializing.')

    self.nodes = nodes
    self.features = features
    self.P = prediction_steps

    # Se è GRU
    if self.is_GRU: 
      self.cell = tf.keras.layers.GRUCell(self.nodes * self.features)
    else:
      self.cell = tf.keras.layers.LSTMCell(self.nodes * self.features)

    # Se ha l'attention
    if self.has_attention:
      self.att = tf.keras.layers.Dense(self.nodes * self.features)

    self.dense = tf.keras.layers.Dense(self.nodes)
    self.logger.info(__name__ + ' initialized.')

  def call(self, inputs):
    B, H, N, F = inputs.shape
    inputs = tf.reshape(inputs, (B, H, N * F)) # [B, H, N*F]

    # Se è Bi-LSTM, concatena la sequenza al contrario
    if self.is_bidirectional:
      reversed = tf.reverse_sequence(inputs, [H for b in range(B)], seq_axis=1, batch_axis=0) # [B, H, N*F]
      inputs = tf.concat([inputs, reversed], 1) # [B, 2*H, N*F]
      H *= 2

    preds = []
    carry = [tf.zeros((B, N * F)), tf.zeros((B, N * F))] # Initial states, matrici di 0 da [B, N*F]
    encoder_states = []

    for h in range(H):
      memory, carry = self.cell(inputs[:,h,:], carry) # Memory: [B, N*F], Carry: [2, [B, N*F]]; memory e carry[0] sono identici
      encoder_states.append(memory) # [h, [B, N*F]]
    encoder_states = tf.stack(encoder_states, axis=1) # controllare [B, H, N*F]

    for p in range(self.P):
      if self.has_attention:
        memory = tf.expand_dims(memory, 1) # [B, 1, N*F]
        scores = tf.matmul(self.att(memory), tf.transpose(encoder_states, perm=[0,2,1]))
        weights = tf.nn.softmax(scores) # [B, 1, H]
        context = tf.matmul(weights, encoder_states)
        context = tf.squeeze(context, axis=1) # [B, N*F]
        memory = context

      memory, carry = self.cell(memory, carry) # Memory: [B, N*F], Carry: [2, [B, N*F]]; memory e carry[0] sono identici
      preds.append(memory) # [p, [B, N*F]]
      
    preds = tf.transpose(preds, perm=[1,0,2]) # [B, P, N*F]
    res = self.dense(preds) # [B, P, N]
    
    return res

  def train_step(self, data):
    x, y = data

    with tf.GradientTape() as tape:
      y_pred = self(x, training=True)
      print(y.shape)
      # Compute the loss value
      loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)

    # Compute gradients and update weights
    trainable_vars = self.trainable_variables
    gradients = tape.gradient(loss, trainable_vars)
    self.optimizer.apply_gradients(zip(gradients, trainable_vars))

    # Update metrics (includes the metric that tracks the loss)
    self.compiled_metrics.update_state(y, y_pred)

    # Return a dict mapping metric names to current value
    return {m.name: m.result() for m in self.metrics}

  def test_step(self, data):
    x, y = data
    y_pred = self(x, training=False)

    # Updates the metrics tracking the loss
    self.compiled_loss(y, y_pred, regularization_losses=self.losses)
    self.compiled_metrics.update_state(y, y_pred)

    # Return a dict mapping metric names to current value.
    return {m.name: m.result() for m in self.metrics}